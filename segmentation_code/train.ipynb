{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from model import build_unet\n",
        "from metrics import dice_loss, dice_coef,iou"
      ],
      "metadata": {
        "id": "Qu_oFH6Z5cUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00mLALHrE0d1",
        "outputId": "6da5697e-2f18-4df2-9183-7fe2f18ac040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Global parameters \"\"\"\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def create_dir(path):\n",
        "    \"\"\" Create a directory. \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ],
      "metadata": {
        "id": "aSdzUlwt5twh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path, split=0.1):\n",
        "    images = sorted(glob(os.path.join(path, \"CXR_png\", \"*.png\")))\n",
        "    masks1 = sorted(glob(os.path.join(path, \"masks\", \"*.png\")))\n",
        "    \n",
        "\n",
        "    split_size = int(len(images) * split)\n",
        "\n",
        "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    train_y1, valid_y1 = train_test_split(masks1, test_size=split_size, random_state=42)\n",
        "  \n",
        "\n",
        "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
        "    train_y1, test_y1 = train_test_split(train_y1, test_size=split_size, random_state=42)\n",
        "    \n",
        "\n",
        "    return (train_x, train_y1), (valid_x, valid_y1), (test_x, test_y1)"
      ],
      "metadata": {
        "id": "vjJqRwN45w7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x"
      ],
      "metadata": {
        "id": "f-li-62k5zhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_mask(path1):\n",
        "    x = cv2.imread(path1, cv2.IMREAD_GRAYSCALE)\n",
        "   \n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/np.max(x)\n",
        "    x = x > 0.5\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yYnW7CBi53Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_parse(x, y1):\n",
        "    def _parse(x, y1):\n",
        "        x = x.decode()\n",
        "        y1 = y1.decode()\n",
        "       \n",
        "\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y1)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y1], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "A6kif-4-58l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_dataset(X, Y1, batch=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y1))\n",
        "    dataset = dataset.shuffle(buffer_size=200)\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(4)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "qteOhheI5_2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory for storing files \"\"\"\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 2\n",
        "    lr = 1e-5\n",
        "    num_epochs = 5\n",
        "    model_path = os.path.join(\"files\", \"model.h5\")\n",
        "    csv_path = os.path.join(\"files\", \"data.csv\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    dataset_path = \"/content/gdrive/My Drive/Covid\"\n",
        "    (train_x, train_y1), (valid_x, valid_y1), (test_x, test_y1) = load_data(dataset_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y1)}\")\n",
        "    print(f\"Valid: {len(valid_x)} - {len(valid_y1)}\")\n",
        "    print(f\"Test: {len(test_x)} - {len(test_y1)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y1, batch=batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y1, batch=batch_size)\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model = build_unet((H, W, 3))\n",
        "    metrics = [dice_coef, iou, Recall(), Precision()]\n",
        "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZNAP6XE6DTb",
        "outputId": "898b452a-a726-4d0b-cd38-9f01da63fae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 80 - 80\n",
            "Valid: 10 - 10\n",
            "Test: 10 - 10\n",
            "Epoch 1/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.5973 - dice_coef: 0.4027 - iou: 0.2549 - recall: 0.5716 - precision: 0.3980 \n",
            "Epoch 1: val_loss improved from inf to 0.64490, saving model to files/model.h5\n",
            "40/40 [==============================] - 2167s 54s/step - loss: 0.5973 - dice_coef: 0.4027 - iou: 0.2549 - recall: 0.5716 - precision: 0.3980 - val_loss: 0.6449 - val_dice_coef: 0.3551 - val_iou: 0.2162 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.4558 - dice_coef: 0.5442 - iou: 0.3759 - recall: 0.8477 - precision: 0.5096 \n",
            "Epoch 2: val_loss did not improve from 0.64490\n",
            "40/40 [==============================] - 2169s 54s/step - loss: 0.4558 - dice_coef: 0.5442 - iou: 0.3759 - recall: 0.8477 - precision: 0.5096 - val_loss: 0.6483 - val_dice_coef: 0.3517 - val_iou: 0.2141 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.3559 - dice_coef: 0.6441 - iou: 0.4779 - recall: 0.8895 - precision: 0.6338 \n",
            "Epoch 3: val_loss did not improve from 0.64490\n",
            "40/40 [==============================] - 2200s 55s/step - loss: 0.3559 - dice_coef: 0.6441 - iou: 0.4779 - recall: 0.8895 - precision: 0.6338 - val_loss: 0.6612 - val_dice_coef: 0.3388 - val_iou: 0.2042 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.2842 - dice_coef: 0.7158 - iou: 0.5593 - recall: 0.8971 - precision: 0.7815 \n",
            "Epoch 4: val_loss did not improve from 0.64490\n",
            "40/40 [==============================] - 2199s 55s/step - loss: 0.2842 - dice_coef: 0.7158 - iou: 0.5593 - recall: 0.8971 - precision: 0.7815 - val_loss: 0.6970 - val_dice_coef: 0.3030 - val_iou: 0.1790 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.2277 - dice_coef: 0.7723 - iou: 0.6317 - recall: 0.9115 - precision: 0.8507 \n",
            "Epoch 5: val_loss did not improve from 0.64490\n",
            "40/40 [==============================] - 2210s 55s/step - loss: 0.2277 - dice_coef: 0.7723 - iou: 0.6317 - recall: 0.9115 - precision: 0.8507 - val_loss: 0.7403 - val_dice_coef: 0.2597 - val_iou: 0.1493 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"iou\", history.history['iou'][-1])\n",
        "print(\"dice\", history.history['dice_coef'][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "qoIBVgw7-4IW",
        "outputId": "0a464d5a-52e7-4f19-82e0-5ce1f50f83ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c82dd54d32cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iou\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iou'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_coef'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ]
}